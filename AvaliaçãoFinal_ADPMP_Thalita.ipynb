{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AvaliaçãoFinal_Thalita.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thalicsouza/icmc_repo/blob/master/Avalia%C3%A7%C3%A3oFinal_ADPMP_Thalita.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flekT6GFDN6m"
      },
      "source": [
        "# <span style=\"color:blue\">MBA em Ciência de Dados</span>\n",
        "# <span style=\"color:blue\">Análise de Dados com Base em Processamento Massivo em Paralelo</span>\n",
        "\n",
        "## <span style=\"color:blue\">Avaliação Final</span>\n",
        "\n",
        "**Material Produzido por:**<br>\n",
        ">**Profa. Dra. Cristina Dutra de Aguiar Ciferri**<br>\n",
        ">**André Perez**<br> \n",
        ">**Guilherme Muzzi da Rocha**<br> \n",
        ">**Jadson José Monteiro Oliveira**<br>\n",
        ">**João Pedro de Carvalho Castro**<br> \n",
        ">**Leonardo Mauro Pereira Moraes**<br> \n",
        ">**Piero Lima Capelo**<br>\n",
        "\n",
        "\n",
        "**CEMEAI - ICMC/USP São Carlos**\n",
        "\n",
        "**A avaliação final contém 7 questões, as quais estão espalhadas ao longo do texto. Por favor, procurem por Questão para encontrar a especificação das questões. Também é possível localizar as questões utilizando o menu de navegação. O *notebook* contém a constelação de fatos da BI Solutions que deve ser utilizada para responder às questões e também toda a obtenção dos dados e a respectiva geração dos DataFrames e das visões temporárias.** \n",
        "\n",
        "**Desejamos uma boa avaliação!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o3dN_WLQcyD"
      },
      "source": [
        "#1 Constelação de Fatos da BI Solutions\n",
        "\n",
        "A aplicação de *data warehousing* da BI Solutions utiliza como base uma contelação de fatos, conforme descrita a seguir.\n",
        "\n",
        "**Tabelas de dimensão**\n",
        "\n",
        "- data (dataPK, dataCompleta, dataDia, dataMes, dataBimestre, dataTrimestre, dataSemestre, dataAno)\n",
        "- funcionario (funcPK, funcMatricula, funcNome, funcSexo, funcDataNascimento, funcDiaNascimento, funcMesNascimento, funcAnoNascimento, funcCidade, funcEstadoNome, funcEstadoSigla, funcRegiaoNome, funcRegiaoSigla, funcPaisNome, funcPaisSigla)\n",
        "- equipe (equipePK, equipeNome, filialNome, filialCidade, filialEstadoNome, filialEstadoSigla, filialRegiaoNome, filialRegiaoSigla, filialPaisNome, filialPaisSigla)\n",
        "- cargo (cargoPK, cargoNome, cargoRegimeTrabalho, cargoEscolaridadeMinima, cargoNivel)\n",
        "- cliente (clientePK, clienteNomeFantasia, clienteSetor, clienteCidade, clienteEstadoNome, clienteEstadoSigla, clienteRegiaoNome, clienteRegiaoSigla, clientePaisNome, clientePaisSigla)\n",
        "\n",
        "**Tabelas de fatos**\n",
        "- pagamento (dataPK, funcPK, equipePK, cargoPK, salario, quantidadeLancamentos)\n",
        "- negociacao (dataPK, equipePK, clientePK, receita, quantidadeNegociacoes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGeh8KdXwVCQ"
      },
      "source": [
        "#2 Obtenção dos Dados da BI Solutions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCCNC64AzBG0"
      },
      "source": [
        "## 2.1 Baixando o Módulo wget\n",
        "\n",
        "Para baixar os dados referentes ao esquema relacional da constelação de fatos da BI Solutions, é utilizado o módulo  **wget**. O comando a seguir realiza a instalação desse módulo. <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e0Eao1K0EYG"
      },
      "source": [
        "#instalando o módulo wget\n",
        "%%capture\n",
        "!pip install -q wget\n",
        "!mkdir data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j56pVJ2hZ2i5"
      },
      "source": [
        "## 2.2 Obtenção dos Dados das Tabelas de Dimensão\n",
        "\n",
        "Os comandos a seguir baixam os dados que povoam as tabelas de dimensão. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46QzTpLJwfkW",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0c1d32d4-491d-41f4-e387-ad814c8ce9af"
      },
      "source": [
        "#baixando os dados das tabelas de dimensão\n",
        "import wget\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/data.csv\"\n",
        "wget.download(url, \"data/data.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/funcionario.csv\"\n",
        "wget.download(url, \"data/funcionario.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/equipe.csv\"\n",
        "wget.download(url, \"data/equipe.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cargo.csv\"\n",
        "wget.download(url, \"data/cargo.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cliente.csv\"\n",
        "wget.download(url, \"data/cliente.csv\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'data/cliente (1).csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o-dC7feszRc"
      },
      "source": [
        "## 2.3 Obtenção dos Dados Tabelas de Fatos\n",
        "\n",
        "Os comandos a seguir baixam os dados que povoam as tabelas de fatos. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWM-CUFgBl_8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b6c044b7-4814-49c4-d733-c7e3164cb5ff"
      },
      "source": [
        "#baixando os dados das tabelas de fatos\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/pagamento.csv\"\n",
        "wget.download(url, \"data/pagamento.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/negociacao.csv\"\n",
        "wget.download(url, \"data/negociacao.csv\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'data/negociacao.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO16-7-jOioq"
      },
      "source": [
        "# 3 Apache Spark Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVEgY9qKflBV"
      },
      "source": [
        "## 3.1 Instalação\n",
        "\n",
        "Neste *notebook* é criado um *cluster* Spark composto apenas por um **nó mestre**. Ou seja, o *cluster* não possui um ou mais **nós de trabalho** e o **gerenciador de cluster**. Nessa configuração, as tarefas (*tasks*) são realizadas no próprio *driver* localizado no **nó mestre**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaM-OnIjgLS2"
      },
      "source": [
        "Para que o cluster possa ser criado, primeiramente é instalado o Java Runtime Environment (JRE) versão 8. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXls3bfoglKW"
      },
      "source": [
        "#instalando Java Runtime Environment (JRE) versão 8\n",
        "%%capture\n",
        "!apt-get remove openjdk*\n",
        "!apt-get update --fix-missing\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BQzZfDYhb4j"
      },
      "source": [
        "Na sequência, é feito o *download* do Apache Spark versão 3.0.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a_Yv59zg3gm"
      },
      "source": [
        "#baixando Apache Spark versão 3.0.0\n",
        "%%capture\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop2.7.tgz && rm spark-3.0.0-bin-hadoop2.7.tgz"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RETWX6wqhkLf"
      },
      "source": [
        "Na sequência, são configuradas as variáveis de ambiente JAVA_HOME e SPARK_HOME. Isto permite que tanto o Java quanto o Spark possam ser encontrados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZpR7NwOh2EB"
      },
      "source": [
        "import os\n",
        "#configurando a variável de ambiente JAVA_HOME\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#configurando a variável de ambiente SPARK_HOME\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql0z7Ro1iHQb"
      },
      "source": [
        "Por fim, são instalados dois pacotes da linguagem de programação Python, cujas funcionalidades são descritas a seguir.\n",
        "\n",
        "> **Pacote findspark:** Usado para ler a variável de ambiente SPARK_HOME e armazenar seu valor na variável dinâmica de ambiente PYTHONPATH. Como resultado, Python pode encontrar a instalação do Spark. \n",
        "\n",
        "> **Pacote pyspark:** PySpark é a API do Python para Spark. Ela possibilita o uso de Python, considerando que o *framework* Apache Spark encontra-se desenvolvido na linguagem de programação Scala. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oSYOwKljPf5"
      },
      "source": [
        "%%capture\n",
        "#instalando o pacote findspark\n",
        "!pip install -q findspark==1.4.2\n",
        "#instalando o pacote pyspark\n",
        "!pip install -q pyspark==3.0.0"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAaLyjPzmIwZ"
      },
      "source": [
        "## 3.2 Conexão\n",
        "\n",
        "PySpark não é adicionado ao *sys.path* por padrão. Isso significa que não é possível importá-lo, pois o interpretador da linguagem Python não sabe onde encontrá-lo. \n",
        "\n",
        "Para resolver esse aspecto, é necessário instalar o módulo `findspark`. Esse módulo mostra onde PySpark está localizado. Os comandos a seguir têm essa finalidade.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zm1pBTEmjp4"
      },
      "source": [
        "#importando o módulo findspark\n",
        "import findspark\n",
        "#carregando a variávels SPARK_HOME na variável dinâmica PYTHONPATH\n",
        "findspark.init()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDqfefF7YUab"
      },
      "source": [
        "Depois de configurados os pacotes e módulos e inicializadas as variáveis de ambiente, é possível iniciar o uso do Spark na aplicação de `data warehousing`. Para tanto, é necessário importar o comando `SparkSession` do módulo `pyspark.sql`. São utilizados os seguintes conceitos: <br>\n",
        "\n",
        "- `SparkSession`: permite a criação de `DataFrames`. Como resultado, as tabelas relacionais podem ser manipuladas por meio de `DataFrames` e é possível realizar consultas OLAP por meio de comandos SQL. <br>\n",
        "- `builder`: cria uma instância de SparkSession. <br>\n",
        "- `appName`: define um nome para a aplicação, o qual pode ser visto na interface de usuário web do Spark. <br> \n",
        "- `master`: define onde está o nó mestre do *cluster*. Como a aplicação é executada localmente e não em um *cluster*, indica-se isso pela *string* `local` seguida do parâmetro `[*]`. Ou seja, define-se que apenas núcleos locais são utilizados. \n",
        "- `getOrCreate`: cria uma SparkSession. Caso ela já exista, retorna a instância existente. \n",
        "\n",
        "\n",
        "**Observação**: A lista completa de todos os parâmetros que podem ser utilizados na inicialização do *cluster* pode ser encontrada neste [link](https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TxljJ_cwBCy"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"pyspark-notebook\").master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qL9SiR_pQE2"
      },
      "source": [
        "# 4 Geração dos DataFrames em Spark da BI Solutions\n",
        "\n",
        "Um `DataFrame` em Spark é equivalente a uma tabela relacional. Portanto, um `DataFrame` possui um esquema, uma ou mais linhas (ou tuplas) e uma ou mais colunas (ou atributos).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRVoz-SGt87W"
      },
      "source": [
        "## 4.1 Criação dos DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "FNR-3dV6oYk4"
      },
      "source": [
        "#criando os DataFrames em Spark \n",
        "cargo = spark.read.csv(path=\"data/cargo.csv\", header=True, sep=\",\")\n",
        "cliente = spark.read.csv(path=\"data/cliente.csv\", header=True, sep=\",\")\n",
        "data = spark.read.csv(path=\"data/data.csv\", header=True, sep=\",\")\n",
        "equipe = spark.read.csv(path=\"data/equipe.csv\", header=True, sep=\",\")\n",
        "funcionario = spark.read.csv(path=\"data/funcionario.csv\", header=True, sep=\",\")\n",
        "negociacao = spark.read.csv(path=\"data/negociacao.csv\", header=True, sep=\",\")\n",
        "pagamento = spark.read.csv(path=\"data/pagamento.csv\", header=True, sep=\",\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrch9vLgjl_H"
      },
      "source": [
        "## 4.2 Atualização dos Tipos de Dados "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A_ot2pOjsWB"
      },
      "source": [
        "Nos comandos a seguir, primeiro são identificados quais colunas de quais `DataFrames` devem ser do tipo de dado inteiro. Na sequência, ocorre a conversão. Por fim, são exibidos os esquemas dos `DataFrames`, possibilitando visualizar a mudança de tipo de dados das colunas especificadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmCV6Mur__z6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c118bbab-08d3-4f41-e0ad-44c0ed5fbb51"
      },
      "source": [
        "# identificando quais colunas de quais DataFrames devem ser do tipo de dado inteiro\n",
        "print('Identificando colunas...')\n",
        "colunas_cargo = [\"cargoPK\"]\n",
        "colunas_cliente = [\"clientePK\"]\n",
        "colunas_data = [\"dataPk\", \"dataDia\", \"dataMes\", \"dataBimestre\", \"dataTrimestre\", \"dataSemestre\", \"dataAno\"]\n",
        "colunas_equipe = [\"equipePK\"]\n",
        "colunas_funcionario = [\"funcPK\", \"funcDiaNascimento\", \"funcMesNascimento\", \"funcAnoNascimento\"]\n",
        "colunas_negociacao = [\"equipePK\", \"clientePK\", \"dataPK\", \"quantidadeNegociacoes\"]\n",
        "colunas_pagamento = [\"funcPK\", \"equipePK\", \"dataPK\", \"cargoPK\", \"quantidadeLancamentos\"]\n",
        "print('Ok!')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Identificando colunas...\n",
            "Ok!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPNnDJcG9R5H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7446b00-cca2-4997-bc26-562abbe42843"
      },
      "source": [
        "# importando o tipo de dado desejado\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import round\n",
        "\n",
        "# atualizando o tipo de dado das colunas especificadas \n",
        "# substituindo as colunas já existentes \n",
        "\n",
        "for coluna in colunas_cargo:\n",
        "  cargo = cargo.withColumn(coluna, cargo[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_cliente:\n",
        "  cliente = cliente.withColumn(coluna, cliente[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_data:\n",
        "  data = data.withColumn(coluna, data[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_equipe:\n",
        "  equipe = equipe.withColumn(coluna, equipe[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_funcionario:\n",
        "  funcionario = funcionario.withColumn(coluna, funcionario[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_negociacao:\n",
        "  negociacao = negociacao.withColumn(coluna, negociacao[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_pagamento:\n",
        "  pagamento = pagamento.withColumn(coluna, pagamento[coluna].cast(IntegerType()))\n",
        "\n",
        "print('Ok!')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ok!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0dX_7U_AzIY"
      },
      "source": [
        "Nos comandos a seguir, primeiro são identificados quais colunas de quais `DataFrames` devem ser do tipo de dado número de ponto flutuante. Na sequência, ocorre a conversão. Por fim, são exibidos os esquemas dos `DataFrames`, possibilitando visualizar a mudança de tipo de dados das colunas especificadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBcQ7Ep7AWqN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1ed4d87-bd5d-4b76-aa01-2fd4c5ab4d9e"
      },
      "source": [
        "# identificando quais colunas de quais DataFrames devem ser do tipo de dado número de ponto flutuante\n",
        "colunas_negociacao = [\"receita\"]\n",
        "colunas_pagamento = [\"salario\"]\n",
        "print('Ok!')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ok!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcfvkIK1BRSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a5f03de-de33-4d97-8e2f-94b980a8f64f"
      },
      "source": [
        "# importando o tipo de dado desejado\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "\n",
        "# atualizando o tipo de dado das colunas especificadas \n",
        "# substituindo as colunas já existentes \n",
        "\n",
        "for coluna in colunas_negociacao:\n",
        "  negociacao = negociacao.withColumn(coluna, negociacao[coluna].cast(FloatType()))\n",
        "\n",
        "for coluna in colunas_pagamento:\n",
        "  pagamento = pagamento.withColumn(coluna, pagamento[coluna].cast(FloatType()))\n",
        "\n",
        "print('Ok!')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ok!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wN5iOGKwnHG"
      },
      "source": [
        "## 4.3 Criação de Visões Temporárias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJsqRI3TwsjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7968ec3-cdb7-4409-bec7-515ab297e55f"
      },
      "source": [
        "#criando as visões temporárias \n",
        "cargo.createOrReplaceTempView(\"cargo\")\n",
        "cliente.createOrReplaceTempView(\"cliente\")\n",
        "data.createOrReplaceTempView(\"data\")\n",
        "equipe.createOrReplaceTempView(\"equipe\")\n",
        "funcionario.createOrReplaceTempView(\"funcionario\")\n",
        "negociacao.createOrReplaceTempView(\"negociacao\")\n",
        "pagamento.createOrReplaceTempView(\"pagamento\")\n",
        "print('Ok!')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ok!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkL4w2MMudL7"
      },
      "source": [
        "# 5 Instruções Importantes sobre a Avaliação\n",
        "\n",
        "Esta avaliação é composta por 7 questões referentes a diferentes consultas OLAP. O valor de cada questão encontra-se especificado juntamente com a definição da questão. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKdDsHnDwlaH"
      },
      "source": [
        "## 5.1 Especificação das Consultas\n",
        "\n",
        "As consultas OLAP devem ser respondidas de acordo com o solicitado em cada questão. As seguintes solicitações podem ser feitas:\n",
        "\n",
        "- Resolva a questão especificando a consulta OLAP na **linguagem SQL**. Neste caso, a consulta deve ser respondida usando os conceitos apresentados na Aula 07 da disciplina. Ou seja, a consulta deve ser respondida usando a linguagem SQL textual e o método `spark.sql()`. Não é possível usar os demais métodos do módulo `pyspark.sql` para especificar a consulta, com exceção do método `show()` para listar o resultado da consulta.  \n",
        "\n",
        "- Resolva a questão especificando a consulta OLAP usando os **métodos de pyspark.sql**. Neste caso, a consulta deve ser respondida usando os conceitos apresentados na Aula 08 da disciplina. Ou seja, a consulta deve ser respondida usando os demais métodos do módulo `pyspark.sql`. Não é possível usar o método `spark.sql()` para especificar a consulta.\n",
        "\n",
        "Caso a consulta seja especificada de forma diferente do que foi solicitado, a resposta não será considerada, mesmo que ela esteja correta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsrYR2SJw-x3"
      },
      "source": [
        "## 5.2 Ordem das Colunas e das Linhas\n",
        "\n",
        "A resolução das questões deve seguir estritamente as especificações definidas em cada consulta. Isto significa que:\n",
        "\n",
        "- As **colunas** solicitadas devem ser exibidas exatamente na mesma ordem que a definida na questão. Note que todas as colunas a serem exibidas como resposta da consulta, bem como a ordem na qual elas devem aparecer são sempre definidas na questão. \n",
        "\n",
        "- As **linhas** retornadas como respostas devem ser exibidas exatamente na mesma ordem que a definida na questão. Note que a ordem na qual as linhas devem aparecer são sempre definidas na questão. \n",
        "\n",
        "- Os **nomes das colunas** renomeadas devem seguir estritamente os nomes definidos na questão. Para evitar possíveis erros, os nomes das colunas renomeadas não possuem acentos e espaços em branco, além de serem escritos utilizando apenas letras maiúsculas. Note que os nomes das colunas renomeadas são sempre definidos na questão.\n",
        "\n",
        "Essas orientações devem ser seguidas uma vez que a correção da avaliação será realizada de forma automática. Caso a consulta retorne resultados de forma diferente do que foi solicitado, a resposta não será considerada, mesmo que ela esteja correta.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AZ0475X4L59"
      },
      "source": [
        "## 5.3 Listagem das Respostas das Consultas\n",
        "\n",
        "A resposta de cada consulta deve ser listada usando o método `show()`. Nenhum outro método pode ser utilizado com essa finalidade.  \n",
        "\n",
        "Devem ser listadas apenas as `20` primeiras linhas de resposta de cada consulta. Adicionalmente, devem ser listadas *strings* com tamanho maior do que 20 caracteres, ou seja, o parâmetro `truncate` do método `show()` deve ser inicializado como `false`.\n",
        "\n",
        "Portanto, a listagem das respostas deve ser feita utilizando o método `show()` como especificado a seguir. \n",
        "\n",
        "- Quando a consulta OLAP for especificada usando a **linguagem SQL**. Utilize o comando `spark.sql(consultaSQL).show(20,truncate=False)` para exibir o resultado da consulta. \n",
        "\n",
        "- Quando a consulta OLAP for especificada usando os demais **métodos de pyspark.sql**. Utilize o comando `nomeDoDataFrame.show(20,truncate=False)` para exibir o resultado da consulta.\n",
        "\n",
        "Por padrão, o método `show()` exibe as `20` primeiras linhas. Mesmo assim, defina o valor `20` como parâmetro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzIcWYeOVOsN"
      },
      "source": [
        "## 5.4 Arredondamento dos Dados\n",
        "\n",
        "Deve ser realizado o arredondamento dos dados todas as vezes que uma função de agregação for aplicada às medidas numéricas `salario` da tabela de dimensão `pagamento` e `receita` da tabela de dimensão `negociacao`. \n",
        "\n",
        "O arredondamento deve ser realizado usando a função `round()` na linguagem SQL e o método `round()` em `pyspark.sql` e deve arredondar os dados até duas casas decimais. Por exemplo, podem ser produzidos resultados da forma `112233.4` e `112233.44`. \n",
        "\n",
        "Portanto, o arredondamento dos dados deve ser feito como especificado a seguir.\n",
        "\n",
        "- Quando a consulta OLAP for especificada usando a **linguagem SQL**. Utilize a função `ROUND(funçãoDeAgregação,2)` para arredondar o dado até duas casas decimais.\n",
        "\n",
        "- Quando a consulta OLAP for especificada usando os demais **métodos de pyspark.sql**. Utilize o método `round(funçãoDeAgregação,2)` para arredondar o dado até duas casas decimais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQVQqm0pNDS1"
      },
      "source": [
        "## 5.5 Comentários Explicativos\n",
        "\n",
        "Devem ser colocados comentários no código que expliquem o passo a passo da resolução da questão. Os comentários explicativos devem ser realizados como especificado a seguir. \n",
        "\n",
        "- Quando a consulta OLAP for especificada usando a **linguagem SQL**. Utilize `#` para colocar comentários gerais (conforme explicado para os demais métodos de `pyspark.sql`) ou utilize `--` para colocar comentários no comando SQL. Por exemplo:\n",
        "\n",
        "```\n",
        "-- na cláusula SELECT são listadas as colunas a serem exibidas\n",
        "SELECT funcNome\n",
        "-- na cláusula FROM são especificadas as relações temporárias\n",
        "FROM funcionario\n",
        "-- na cláusula WHERE são definidas as condições de seleção\n",
        "WHERE funcPK = 1\n",
        "```\n",
        "\n",
        "- Quando a consulta OLAP for especificada usando os demais **métodos de pyspark.sql**. Utilize `#` para colocar comentário. Por exemplo:\n",
        "\n",
        "```\n",
        "# no comando a seguir, é aplicado o método select() sobre o DataFrame \n",
        "# para listar as colunas a serem exibidas, depois é aplicado o método\n",
        "# filter() para listar as condições de seleção  \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IcbpvZHPn3S"
      },
      "source": [
        "## 5.6 Indentação e Organização\n",
        "\n",
        "As consultas e os comandos que respondem às questões dessa avaliação devem ser escritos de forma indentada. Em caso de dúvida, observem os *notebooks* da Aula 07 e da Aula 08 e verifiquem como as consultas e os comandos foram indentados.\n",
        "\n",
        "Com relação à organização, é necessário que as respostas às questões sejam localizadas aonde especificado no *notebook*. Por favor, procurem por \"Resposta da Questão\" para encontrar o local no qual as respostas devem ser especificadas. Também é possível localizar o local das respostas utilizando o menu de navegação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFjCivy1Mg-A"
      },
      "source": [
        "## 5.7 Critério de Avaliação\n",
        "\n",
        "Na correção da avaliação, serão ponderados os seguintes aspectos:\n",
        "\n",
        "- Corretude da execução das consultas OLAP.\n",
        "\n",
        "- Atendimento às especificações definidas nas seções 5.1, 5.2, 5.3 e 5.4.\n",
        "\n",
        "- Atendimento às especificações da sintaxe das cláusulas e dos métodos utilizados para resolver cada questão.\n",
        "\n",
        "- Qualidade da documentação entregue, de acordo com as especificações definidas nas seções 5.5 e 5.6. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss0pmgplPAL3"
      },
      "source": [
        "# 6 Questões"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_emI31_-uYc5"
      },
      "source": [
        "O mercado de trabalho brasileiro usualmente mostra que as mulheres ainda não possuem o mesmo reconhecimento que os homens. Existem diversas pesquisas que mostram que as mulheres ganham menos que homens em todos os cargos, áreas de atuação e níveis de escolaridade. Além disso, mulheres ainda são minoria quando consideradas posições nos principais cargos de gestão. Adicionalmente, existem estudos que indicam que a participação feminina no mercado de trabalho brasileiro aumenta a produtividade. \n",
        "\n",
        "O objetivo da avaliação é investigar se existe disparidade entre o sexo feminino e masculino na BI Solutions. São considerados os seguintes aspectos nas análises a serem realizadas: temporalidade e regionalidade. \n",
        "\n",
        "Os resultados obtidos na avaliação poderão ser posteriormente utilizados para definir estratégias que a BI Solutions deve executar para resolver essa disparidade, caso necessário. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLD9FAdWK-Sp"
      },
      "source": [
        "## 6.1 Visão Comparativa Relacionada aos Sexos\n",
        "\n",
        "O objetivo das análises desta seção é obter uma visão relacionada aos sexos, por meio da comparação da média dos salários recebidos por mulheres e homens. Podem ser realizadas diferentes análises, sendo que duas delas são solicitadas a seguir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kUBwA4WKmQ_"
      },
      "source": [
        "### Questão 1 \n",
        "\n",
        "**(valor: 1,0)** Liste, para cada `dataAno` e para cada sexo do funcionário, a média dos salários. Arredonde a média dos salários para até duas casas decimais. Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: \"ANO\", \"SEXO\" e \"MEDIASALARIO\". Ordene as linhas exibidas primeiro por ano e depois por sexo, todos em ordem ascendente. Liste as primeiras 20 linhas da resposta, sem truncamento das *strings*.\n",
        "\n",
        "**Resolva a questão especificando a consulta OLAP na linguagem SQL**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsWybehcoEvc"
      },
      "source": [
        "### Resposta da Questão 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLOmAqmOoMvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d506ce3d-8781-4e6e-ec02-2c04ef8adcc0"
      },
      "source": [
        "# Resposta da Questão 1 \n",
        "\n",
        "\n",
        "query = \"\"\"\n",
        "--selecionando as colunas solicitadas de ano de referência, sexo do funcionário e respectiva média salarial\n",
        "SELECT dataAno AS `ANO`, \n",
        "       funcSexo AS `SEXO`, \n",
        "       ROUND(AVG(salario), 2) AS `MEDIASALARIO` --arredondando o salário para que contenha 2 casas decimais\n",
        "\n",
        "FROM pagamento JOIN data ON data.dataPK = pagamento.dataPK --unificando os dataframes pagamento e data por meio da chave dataPK, presente nas duas tabelas\n",
        "               JOIN funcionario ON funcionario.funcPK = pagamento.funcPK --unificando os dataframes pagamento e funcionario por meio da chave funcPK, presente nas duas tabelas\n",
        "GROUP BY dataAno, funcSexo --agregando os resultados por ano e gênero do funcionário\n",
        "ORDER BY dataAno, funcSexo --ordenando os resultados de forma crescente por ano e gênero(ordem alfabética da sigla) do funcionário\n",
        "\"\"\"\n",
        "\n",
        "#exibindo as 20 primeiras linhas da consulta\n",
        "spark.sql(query).show(20,truncate=False) \n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----+------------+\n",
            "|ANO |SEXO|MEDIASALARIO|\n",
            "+----+----+------------+\n",
            "|2016|F   |9169.09     |\n",
            "|2016|M   |6905.97     |\n",
            "|2017|F   |8335.68     |\n",
            "|2017|M   |7131.29     |\n",
            "|2018|F   |8333.79     |\n",
            "|2018|M   |7605.44     |\n",
            "|2019|F   |7584.83     |\n",
            "|2019|M   |7789.15     |\n",
            "|2020|F   |7584.83     |\n",
            "|2020|M   |7789.15     |\n",
            "+----+----+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddwc0fdWQpL1"
      },
      "source": [
        "### Questão 2\n",
        "\n",
        "**(valor: 1,0)** Liste todas as agregações que podem ser geradas a partir da média dos salários dos funcionários por `dataAno` por `funcSexo` por `funcRegiaoNome`. Arredonde a média dos salários para até duas casas decimais. Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: \"ANO\", \"SEXO\", \"REGIAO\", \"MEDIASALARIO\". Ordene as linhas exibidas primeiro por ano, depois por sexo, depois por nome da região, todos em ordem ascendente. Liste as primeiras 20 linhas da resposta, sem truncamento das *strings*.\n",
        "\n",
        "**Resolva a questão especificando a consulta OLAP usando os métodos de pyspark.sql**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tfKPSOfolLI"
      },
      "source": [
        "### Resposta da Questão 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBQut6HeaWGf",
        "outputId": "7db74eed-7490-4ed1-e011-ea2c6e92b863"
      },
      "source": [
        "# Resposta da questão 2\n",
        "\n",
        "#favor considerar um comentário por linha, na ordem em q aparecem na consulta (estava quebrando o código, por isso fiz dessa forma)\n",
        "#join da tb funcionario com a tb pagamento,chave=funcPK\n",
        "#join da tb funcionario com a tb data, chave=dataPK\n",
        "#selecionando as colunas solicitadas de ano de referência, sexo e região do funcionário, e respectiva média salarial\n",
        "#aplicando a extensão Cube, que cria subtotais de nível mais alto, de forma progressiva, obtendo assim todas as combinações dos atributos\n",
        "#renomeando a coluna de ano\n",
        "#renomeando a coluna de região\n",
        "#arredondando a coluna de média salarial\n",
        "#renomeando a coluna de média salarial\n",
        "#ordenando de forma crescente pelas colunas de ano, sexo e região\n",
        "#exibindo as primeiras 20 linhas da consulta\n",
        "\n",
        "funcionario.join(pagamento, on=\"funcPK\")\\\n",
        "   .join(data, 'dataPK', 'inner')\\\n",
        "   .select(\"dataAno\",\"funcSexo\",\"funcRegiaoNome\",\"salario\")\\\n",
        "   .cube(\"dataAno\",\"funcSexo\",\"funcRegiaoNome\").avg(\"salario\")\\\n",
        "   .withColumnRenamed(\"dataAno\", \"ANO\")\\\n",
        "   .withColumnRenamed(\"funcSexo\", \"SEXO\")\\\n",
        "   .withColumnRenamed(\"funcRegiaoNome\", \"REGIAO\")\\\n",
        "   .withColumn(\"avg(salario)\", round(\"avg(salario)\",2))\\\n",
        "   .withColumnRenamed(\"avg(salario)\", \"MEDIASALARIO\")\\\n",
        "   .orderBy(\"dataAno\",\"funcSexo\",\"funcRegiaoNome\")\\\n",
        "   .show(20, truncate=False)\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----+--------+------------+\n",
            "|ANO |SEXO|REGIAO  |MEDIASALARIO|\n",
            "+----+----+--------+------------+\n",
            "|null|null|null    |7671.81     |\n",
            "|null|null|NORDESTE|7151.26     |\n",
            "|null|null|SUDESTE |7445.65     |\n",
            "|null|null|SUL     |9397.17     |\n",
            "|null|F   |null    |7948.08     |\n",
            "|null|F   |NORDESTE|7043.0      |\n",
            "|null|F   |SUDESTE |7820.93     |\n",
            "|null|F   |SUL     |9282.58     |\n",
            "|null|M   |null    |7581.08     |\n",
            "|null|M   |NORDESTE|7183.33     |\n",
            "|null|M   |SUDESTE |7321.75     |\n",
            "|null|M   |SUL     |9436.46     |\n",
            "|2016|null|null    |7403.86     |\n",
            "|2016|null|NORDESTE|9350.75     |\n",
            "|2016|null|SUDESTE |6542.14     |\n",
            "|2016|null|SUL     |14505.0     |\n",
            "|2016|F   |null    |9169.09     |\n",
            "|2016|F   |SUDESTE |8586.8      |\n",
            "|2016|F   |SUL     |14992.0     |\n",
            "|2016|M   |null    |6905.97     |\n",
            "+----+----+--------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBBSsa9WlJ2f"
      },
      "source": [
        "## 6.2 Visão Específica da Atuação Feminina\n",
        "\n",
        "O objetivo das análises desta seção é obter uma visão direcionada especificamente à atuação feminina, considerando aspectos individuais referentes a salários e receitas. Podem ser realizadas diferentes análises, sendo que duas delas são solicitadas a seguir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6QZzSzGbXQI"
      },
      "source": [
        "### Questão 3\n",
        "\n",
        "**(valor 1,5)** Liste, para cada `dataAno`, a soma dos salários das funcionárias do sexo feminino que nasceram entre os anos de 1970 (inclusive) e 1990 (inclusive) e que moram na região \"SUDESTE\" (\"SE\") ou \"NORDESTE\" (\"NE\"). Arredonde a soma dos salários para até duas casas decimais. Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: \"ANO\", \"IDADE\", \"REGIAO\" e \"TOTALSALARIO\". Ano corresponde ao atributo `dataAno` da tabela de dimensão `data`, idade corresponde ao cálculo feito considerando o ano atual de 2020 e o atributo `funcAnoNascimento` da tabela de dimensão `funcionario`, região corresponde ao atributo `funcRegiaoNome` da tabela de dimensão `funcionario`. Ordene as linhas exibidas primeiro por ano, depois por idade e depois por região, todos em ordem ascendente. Liste as primeiras 20 linhas da resposta, sem truncamento das *strings*.\n",
        "\n",
        "**Resolva a questão especificando a consulta OLAP na linguagem SQL**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqsvVdf9pATO"
      },
      "source": [
        "### Resposta da Questão 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Blhjz07pDGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e6eb9de-1da3-455b-c430-1f5abc0d1da1"
      },
      "source": [
        "# Resposta da Questão 3\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT b.dataAno AS `ANO`, --selecionando as colunas solicitadas de ano de referência, idade atual do funcionário, região e salário total\n",
        "       (2020 - c.funcAnoNascimento) AS `IDADE`,\n",
        "       c.funcRegiaoNome AS `REGIAO`,\n",
        "       ROUND(SUM(a.salario),2) AS `TOTALSALARIO` --arredondando o salário para que contenha 2 casas decimais\n",
        "FROM pagamento a JOIN data b ON a.dataPK = b.dataPK --unificando os dataframes pagamento e data por meio da chave dataPK, presente nas duas tabelas\n",
        "                 JOIN funcionario c ON a.funcPk = c.funcPk --unificando os dataframes pagamento e funcionario por meio da chave funcPK, presente nas duas tabelas\n",
        "WHERE c.funcSexo == 'F' --condição para selecionar apenas o sexo feminino\n",
        "AND c.funcAnoNascimento BETWEEN 1970 AND 1990 --condição para selecionar apenas funcionárias que nasceram entre 1970 e 1990\n",
        "AND c.funcRegiaoNome IN ('SUDESTE','NORDESTE') --condição para selecionar apenas as funcionárias das regiões Sudeste e NOrdeste\n",
        "GROUP BY ANO, IDADE, REGIAO --agregando resultados por ano, idade e região\n",
        "ORDER BY ANO, IDADE, REGIAO --ordenando resultados de forma crescente por ano, idade e região\n",
        "\"\"\"\n",
        "spark.sql(query).show(20, truncate=False)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+--------+------------+\n",
            "|ANO |IDADE|REGIAO  |TOTALSALARIO|\n",
            "+----+-----+--------+------------+\n",
            "|2016|30   |SUDESTE |342132      |\n",
            "|2016|47   |SUDESTE |172356      |\n",
            "|2017|30   |NORDESTE|21564       |\n",
            "|2017|30   |SUDESTE |1330572     |\n",
            "|2017|46   |SUDESTE |53736       |\n",
            "|2017|47   |SUDESTE |196164      |\n",
            "|2018|30   |NORDESTE|70908       |\n",
            "|2018|30   |SUDESTE |1768572     |\n",
            "|2018|35   |SUDESTE |113652      |\n",
            "|2018|46   |SUDESTE |53736       |\n",
            "|2018|47   |SUDESTE |196164      |\n",
            "|2019|30   |NORDESTE|416736      |\n",
            "|2019|30   |SUDESTE |1927860     |\n",
            "|2019|34   |SUDESTE |59736       |\n",
            "|2019|35   |SUDESTE |113652      |\n",
            "|2019|46   |SUDESTE |53736       |\n",
            "|2019|47   |SUDESTE |196164      |\n",
            "|2020|30   |NORDESTE|416736      |\n",
            "|2020|30   |SUDESTE |1927860     |\n",
            "|2020|34   |SUDESTE |59736       |\n",
            "+----+-----+--------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE-uEzVVm5Pk"
      },
      "source": [
        "### Questão 4 \n",
        "\n",
        "**(valor 1,5)** Considere que as equipes cujos valores de `equipePK` são iguais a `1, 3 e 5` possuem a maior quantidade de funcionárias do sexo feminino. Liste, para cada `dataAno`, a soma das receitas recebidas por essas equipes, o nome da equipe, o nome da filial e o setor do cliente, considerando apenas os clientes localizados na cidade de \"SAO CARLOS\". Arredonde a média dos salários para até duas casas decimais. Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: \"ANO\", \"NOMEEQUIPE\", \"NOMEFILIAL\", \"SETORCLIENTE\", \"TOTALRECEITA\". Ordene as linhas exibidas primeiro por ano, depois por nome da equipe, depois por nome da filial e depois por setor do cliente, todos em ordem ascendente. Liste as primeiras 20 linhas da resposta, sem truncamento das *strings*.\n",
        "\n",
        "**Resolva a questão especificando a consulta usando os métodos de pyspark.sql**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5h4J6hCpf79"
      },
      "source": [
        "### Resposta da Questão 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9hGBhf2jU-g"
      },
      "source": [
        "#favor considerar um comentário por linha, na ordem em q aparecem na consulta (estava quebrando o código, por isso fiz dessa forma)\n",
        "#join da tb negociação com a tb data,chave=dataPK\n",
        "#join da tb negociação com a tb equipe, chave=equipePK\n",
        "#join da tb negociação com a tb cliente, chave=clientePK\n",
        "#aplicando a condição para selecionar apenas as equipes com equipePK==1,3,5, e o cidade do cliente==São Carlos\n",
        "#selecionando as colunas solicitadas de ano de referência, nome da equipe, nome da filial, setor do cliente e receita\n",
        "#arredondando a coluna de receita\n",
        "#renomeando a coluna de ano\n",
        "#renomeando a coluna de equipe\n",
        "#renomeando a coluna de setor do cliente\n",
        "#renomeando a coluna de receita total\n",
        "#ordenando de forma crescente pelas colunas de ano, equipe, filial e setor do cliente\n",
        "#exibindo as primeiras 20 linhas da consulta\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIkLfuA-kOls",
        "outputId": "4c0fd10f-2855-4863-99b6-46a04b91638a"
      },
      "source": [
        "negociacao.join(data, 'dataPK', 'inner')\\\n",
        "   .join(equipe, 'equipePK', 'inner')\\\n",
        "   .join(cliente, 'clientePK', 'inner')\\\n",
        "   .where(\"equipePK in (1,3,5) and clienteCidade=='SAO CARLOS'\")\\\n",
        "   .select(\"dataAno\",\"equipeNome\",\"filialNome\",\"ClienteSetor\",\"receita\")\\\n",
        "   .groupBy(\"dataAno\",\"equipeNome\",\"filialNome\",\"ClienteSetor\").sum(\"receita\")\\\n",
        "   .withColumn(\"sum(receita)\", round(\"sum(receita)\",2))\\\n",
        "   .withColumnRenamed(\"dataAno\", \"ANO\")\\\n",
        "   .withColumnRenamed(\"equipeNome\", \"NOMEEQUIPE\")\\\n",
        "   .withColumnRenamed(\"filialNome\", \"NOMEFILIAL\")\\\n",
        "   .withColumnRenamed(\"ClienteSetor\", \"SETORCLIENTE\")\\\n",
        "   .withColumnRenamed(\"sum(receita)\", \"TOTALRECEITA\")\\\n",
        "   .orderBy(\"dataAno\",\"equipeNome\",\"filialNome\",\"ClienteSetor\")\\\n",
        "   .show(20, truncate=False)\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-------------+------------------------+-------------------+------------+\n",
            "|ANO |NOMEEQUIPE   |NOMEFILIAL              |SETORCLIENTE       |TOTALRECEITA|\n",
            "+----+-------------+------------------------+-------------------+------------+\n",
            "|2016|APP - DESKTOP|SAO PAULO - AV. PAULISTA|BEBIDAS E ALIMENTOS|82203       |\n",
            "|2016|APP - DESKTOP|SAO PAULO - AV. PAULISTA|VESTUARIO          |71007       |\n",
            "|2017|APP - DESKTOP|SAO PAULO - AV. PAULISTA|BEBIDAS E ALIMENTOS|11256       |\n",
            "|2017|APP - DESKTOP|SAO PAULO - AV. PAULISTA|VESTUARIO          |103027      |\n",
            "|2017|WEB          |CAMPO GRANDE - CENTRO   |TECNOLOGIA         |57953       |\n",
            "|2017|WEB          |CAMPO GRANDE - CENTRO   |VESTUARIO          |12274       |\n",
            "|2017|WEB          |SAO PAULO - AV. PAULISTA|TECNOLOGIA         |3602        |\n",
            "|2017|WEB          |SAO PAULO - AV. PAULISTA|VESTUARIO          |37812       |\n",
            "|2018|APP - DESKTOP|SAO PAULO - AV. PAULISTA|BEBIDAS E ALIMENTOS|12383       |\n",
            "|2018|APP - DESKTOP|SAO PAULO - AV. PAULISTA|VESTUARIO          |76043       |\n",
            "|2018|WEB          |CAMPO GRANDE - CENTRO   |TECNOLOGIA         |26722       |\n",
            "|2018|WEB          |CAMPO GRANDE - CENTRO   |VESTUARIO          |970         |\n",
            "|2018|WEB          |SAO PAULO - AV. PAULISTA|TECNOLOGIA         |31092       |\n",
            "|2018|WEB          |SAO PAULO - AV. PAULISTA|VESTUARIO          |16979       |\n",
            "|2019|APP - DESKTOP|SAO PAULO - AV. PAULISTA|BEBIDAS E ALIMENTOS|37909       |\n",
            "|2019|APP - DESKTOP|SAO PAULO - AV. PAULISTA|VESTUARIO          |32698       |\n",
            "|2019|WEB          |CAMPO GRANDE - CENTRO   |TECNOLOGIA         |8520        |\n",
            "|2019|WEB          |CAMPO GRANDE - CENTRO   |VESTUARIO          |514         |\n",
            "|2019|WEB          |SAO PAULO - AV. PAULISTA|TECNOLOGIA         |16136       |\n",
            "|2019|WEB          |SAO PAULO - AV. PAULISTA|VESTUARIO          |1201        |\n",
            "+----+-------------+------------------------+-------------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cif1-kov8bym"
      },
      "source": [
        "## 6.3 Visão Geral da Atuação Feminina\n",
        "\n",
        "O objetivo das análises desta seção é obter uma visão direcionada especificamente à atuação feminina, considerando aspectos conjuntos referentes a salários e receitas. Podem ser realizadas diferentes análises, dentre as quais destaca-se a análise base descrita a seguir.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRagvNvcWX3Q"
      },
      "source": [
        "### **Análise Base** \n",
        "\n",
        "Liste, para cada `dataAno`, a soma dos salários das funcionárias de sexo feminino que moram no estado do \"RIO DE JANEIRO\" (\"RJ\") e as somas das receitas recebidas pelas equipes localizadas no estado do \"RIO DE JANEIRO\" (\"RJ\"). O estado no qual as funcionárias moram pode ser identificado pelos atributos `funcEstadoNome` ou `funcEstadoSigla` da tabela de dimensão `funcionario`, enquanto que o estado nos quais as equipes estão localizadas pode ser identificado pelos atributos `filialEstadoNome` ou `filialEstadoSigla` da tabela de dimensão `equipe`. Arredonde a soma dos salários e a soma das receitas para até duas casas decimais. Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: \"ANO\", \"TOTALSALARIO\", \"TOTALRECEITA\". Ordene as linhas exibidas por ano em ordem ascendente. Liste as primeiras 20 linhas da resposta, sem truncamento das *strings*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr4W91rckYJJ"
      },
      "source": [
        "### Questão 5\n",
        "**(valor: 1,5) Resolva a \"Análise Base\" especificando a consulta OLAP na linguagem SQL**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYjwVTL3p1Jc"
      },
      "source": [
        "### Resposta da Questão 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B7n3riYp9bj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce2ae0d-5d03-45c3-f1f3-73ff1d5c8aca"
      },
      "source": [
        "# Resposta da Questão 5\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT anoPag as ANO, --selecionando as colunas solicitadas de ano de pagto, salário total, receita total\n",
        "       ROUND(salario,2) AS `TOTALSALARIO`, \n",
        "       ROUND(receita,2) AS `TOTALRECEITA`\n",
        "FROM (SELECT c.dataAno, --criando uma subquery para consultar data de referência, salário total\n",
        "      ROUND(sum(b.salario),2) AS salario \n",
        "      FROM funcionario a JOIN pagamento b ON a.funcPK = b.funcPK --unificando os dataframes funcionario e pagamento por meio da chave funcPK, presente nas duas tabelas\n",
        "                         JOIN data c ON b.dataPK = c.dataPK --unificando os dataframes funcionario e data por meio da chave dataPK, presente nas duas tabelas\n",
        "      WHERE a.funcSexo = 'F' --condição para selecionar apenas funcionários do sexo feminino\n",
        "      AND a.funcEstadoNome = 'RIO DE JANEIRO' --condição para selecionar apenas funcionárias do RJ\n",
        "      GROUP BY c.dataAno) AS pag(anoPag, salario) --agregando resultados por ano\n",
        "JOIN (SELECT a.dataAno,\n",
        "      ROUND(sum(b.receita),2) AS TOTALRECEITA \n",
        "      FROM data a JOIN negociacao b ON a.dataPK = b.dataPK \n",
        "                  JOIN equipe c ON b.equipePK = c.equipePK\n",
        "WHERE c.filialEstadoNome = 'RIO DE JANEIRO'\n",
        "GROUP BY a.dataAno) AS neg(anoNeg, receita) ON anoPag = anoNeg --agregando resultados por ano de referência\n",
        "ORDER BY anoPag --ordenando resultados de forma crescente por ano de pagamento\n",
        "\"\"\"\n",
        "spark.sql(query).show(20, truncate=False) #exibindo as primeiras 20 linhas da consulta\n"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+------------+------------+\n",
            "|ANO |TOTALSALARIO|TOTALRECEITA|\n",
            "+----+------------+------------+\n",
            "|2016|30060       |2204981     |\n",
            "|2017|30060       |3484865     |\n",
            "|2018|48096       |4741037     |\n",
            "|2019|70776       |5100814     |\n",
            "|2020|70776       |4192269     |\n",
            "+----+------------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6NnIh3qE3zE"
      },
      "source": [
        "###  Questão 6 \n",
        "\n",
        "**(valor: 1,5) Resolva a \"Análise Base\" especificando a consulta OLAP usando os métodos de pyspark.sql**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbMGkFxAqEwP"
      },
      "source": [
        "### Resposta da Questão 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae1RNug_sdE2",
        "outputId": "f5d62dac-c1b3-4cfb-f5c5-17bfbfa1d077"
      },
      "source": [
        "# Resposta da Questão 6\n",
        "\n",
        "#selecionando data de referência e salário e armazenando no objeto pagFunc\n",
        "pagFunc = funcionario.join(pagamento, \"funcPK\")\\\n",
        "    .join(data, \"dataPK\")\\\n",
        "    .where(\"funcSexo='F' AND funcEstadoNome='RIO DE JANEIRO'\")\\\n",
        "    .select(\"dataAno\",\"salario\")\\\n",
        "    .groupBy(\"dataAno\")\\\n",
        "    .sum(\"salario\")\\\n",
        "    .withColumn(\"sum(salario)\", round(\"sum(salario)\",2))\\\n",
        "    .orderBy(\"dataAno\")\\\n",
        "\n",
        "#selecionando data de referência e receita e armazenando no objeto negFil\n",
        "negFil = data.join(negociacao,\"dataPK\")\\\n",
        "    .join(equipe, \"equipePK\")\\\n",
        "    .where(\"filialEstadoNome = 'RIO DE JANEIRO'\")\\\n",
        "    .select(\"dataAno\", \"receita\")\\\n",
        "    .groupBy(\"dataAno\")\\\n",
        "    .sum(\"receita\")\\\n",
        "    .withColumn(\"sum(receita)\", round(\"sum(receita)\",2))\\\n",
        "    .orderBy(\"dataAno\")\\\n",
        "\n",
        "#unificando as duas consultas para trazer o resultado final\n",
        "pagFunc.join(negFil, \"dataAno\")\\\n",
        "   .select(\"dataAno\", \"sum(salario)\", \"sum(receita)\")\\\n",
        "   .orderBy(\"dataAno\")\\\n",
        "   .withColumn(\"sum(salario)\", round(\"sum(salario)\",2))\\\n",
        "   .withColumn(\"sum(receita)\", round(\"sum(receita)\",2))\\\n",
        "   .withColumnRenamed(\"sum(salario)\", \"TOTALSALARIO\")\\\n",
        "   .withColumnRenamed(\"sum(receita)\", \"TOTALRECEITA\")\\\n",
        "   .show(20, truncate=False) #exibindo as primeiras 20 linhas da consulta\n"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------------+------------+\n",
            "|dataAno|TOTALSALARIO|TOTALRECEITA|\n",
            "+-------+------------+------------+\n",
            "|2016   |30060       |2204981     |\n",
            "|2017   |30060       |3484865     |\n",
            "|2018   |48096       |4741037     |\n",
            "|2019   |70776       |5100814     |\n",
            "|2020   |70776       |4192269     |\n",
            "+-------+------------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWCX859nDwG3"
      },
      "source": [
        "## 6.4 Visão Comparativa Final\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oadu9w10TfFq"
      },
      "source": [
        "O objetivo da análise desta seção é obter uma visão relacionada aos sexos, por meio da comparação do total anual de gastos em salários para o pagamento das mulheres e dos homens em comparação ao total anual de receitas recebidas. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9ccL9R-QPuT"
      },
      "source": [
        "### Questão 7\n",
        "\n",
        "**(valor 2,0)** Liste, para cada `dataAno`, a soma dos salários das funcionárias de sexo feminino, a soma dos salários dos funcionários do sexo masculino e as somas das receitas recebidas. Arredonde a soma dos salários e a soma das receitas para até duas casas decimais. Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: \"ANO\", \"TOTALSALARIOMULHERES\", \"TOTALSALARIOHOMENS\", \"TOTALRECEITA\". Ordene as linhas exibidas por ano em ordem ascendente. Liste as primeiras 20 linhas da resposta, sem truncamento das *strings*.\n",
        "\n",
        "**Resolva a questão especificando a consulta OLAP na linguagem SQL**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFOQQmdbqbft"
      },
      "source": [
        "### Resposta da Questão 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrJNZCS2qeE7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e91fc7c7-7f31-4610-989b-5c30f9aeaf24"
      },
      "source": [
        "# Resposta da Questão 7\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT anoPag as ANO, --selecionando as colunas solicitadas de ano de pagto, salário (homens e mulheres), receita total, com arredondamento de casas decimais\n",
        "       ROUND(salarioM, 2) AS `TOTALSALARIOMULHERES`,\n",
        "       ROUND(salarioH, 2) AS `TOTALSALARIOHOMENS`,\n",
        "       ROUND(receita, 2) AS `TOTALRECEITA`\n",
        "FROM (SELECT c.dataAno, \n",
        "      ROUND(sum(b.salario), 2) AS `TOTALSALARIOMULHERES` --selecionando via subquery a soma total dos salários das mulheres\n",
        "      FROM funcionario a JOIN pagamento b ON a.funcPK = b.funcPK \n",
        "                         JOIN data c ON b.dataPK = c.dataPK\n",
        "      WHERE a.funcSexo = 'F' \n",
        "      GROUP BY c.dataAno) AS `TOTALSALARIOMULHERES`(anoPag, salarioM)\n",
        "JOIN (SELECT a.dataAno, --fazendo join com subquery para trazer o valor da receita das equipes\n",
        "      ROUND(SUM(b.receita), 2)  \n",
        "      FROM data a JOIN negociacao b ON a.dataPK = b.dataPK \n",
        "                  JOIN equipe c ON b.equipePK = c.equipePK\n",
        "      GROUP BY a.dataAno) AS `TOTALRECEITA`(anorec, receita) ON anoPag = anorec\n",
        "JOIN (SELECT c.dataAno, --fazendo join com subquery para trazer a soma total dos salários dos homens\n",
        "      ROUND(SUM(b.salario), 2)  \n",
        "      FROM funcionario a JOIN pagamento b ON a.funcPK = b.funcPK \n",
        "                         JOIN data c ON b.dataPK = c.dataPK\n",
        "      WHERE a.funcSexo = 'M' \n",
        "      GROUP BY c.dataAno) AS `TOTALSALARIOHOMENS`(anoPagH, salarioH) ON anoPag = anoPagH --agregando resultados por ano\n",
        "ORDER BY ANO --ordenando resultados por ano\n",
        "\"\"\"\n",
        "spark.sql(query).show(20, truncate=False) #exibindo as primeiras 20 linhas da consulta"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+--------------------+------------------+------------+\n",
            "|ANO |TOTALSALARIOMULHERES|TOTALSALARIOHOMENS|TOTALRECEITA|\n",
            "+----+--------------------+------------------+------------+\n",
            "|2016|1210320             |3231996           |4614114     |\n",
            "|2017|2500704             |7273920           |7200173     |\n",
            "|2018|3800208             |11134368          |11593146    |\n",
            "|2019|4732932             |13833528          |35352737    |\n",
            "|2020|4732932             |13833528          |30221639    |\n",
            "+----+--------------------+------------------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVl10P3QxwdS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}